\chapter{Conclusion}
\label{chapter:conclusion}

\section{Key Results}

In this thesis methods for combining a stereo camera with an omnidirectional camera in order to do visual SLAM were discussed.  Using the omni camera in order to improve the instances and quality of loop closures was identified as the best approach to improve accuracy on a SLAM system.  A loop closure pipeline was developed to make use of the omni camera to identify and calculate loop closure constraints taking advantage of the wide field of view, allowing loop closures to occur invariant to the change of pose of the cameras in both frames.

Imagery from the omni camera was used in a place recognition module to identify potential loop closures.  Having found an potential match, a geometry check was performed on the pair of triplet images.  Two methods were identified for performing the geometry check and returning a metric scale transform that can be added to the SLAM graph. First by utilizing the p3p algorithm to find a transformation between 2D-3D matches, 3D points being generated by stereo and 2D points from the omni images.  The second approach was by calculating an arbitrary non metric scale transformation using the omni camera images and correcting this transform to metric scale using stereo data.  These methods were both implemented and compared.

This loop closure pipeline was integrated into an existing stereo SLAM to evaluate its effectiveness. The modified SLAM system was shown to have an improvement of accuracy of up to 82\%.

In addition to the loop closure results, a set of calibration tools were developed and presented to determine the extrinsic calibration between the omni camera and the stereo camera required by the loop closure pipeline.

\section{Contributions}

The contributions of this work may be summarized as follows:

\begin{itemize}
 \item A novel method for combining stereo and omni cameras to perform loop closures was presented.  This method was shown to improve the accuracy of a stereo SLAM system by 82\%.
 \item An extrinsic calibration approach to calibrate a stereo camera to an omni camera for use with the loop closure pipeline was given.  The non linear calibration was shown to produce a high accuracy calibration even with limited field of view overlap between the two cameras.
 \item A modular system architecture to integrate the loop closure pipeline to an existing stereo SLAM system was also outlined.  Given a SLAM system that interfaces with ROS, this architecture could be used to augment the SLAM system with the presented loop closure pipeline.
 \item A ROS wrapper for ScaViSLAM was developed
\end{itemize}

\section{Future Work}

%TODO: place recogs citation

There are a number of different aspects that could be identified for future work.  The most obvious improvement could be obtained by focusing on the place recognition component.  The place recognizer used in this work was a rather primitive implementation and produced a lot of false positives.  Using the architecture described in this work, it would actually be very easy to swap it out with another existing proven algorithm, such as Fabmap or the DIRD based place recognition library.

There are also alternatives to using image based place recognition, such as using GPS.  This should produce much more accurate loop closure candidates.  Another much simpler option is to select keyframes based on euclidean distance between keyframes in the SLAM graph.  To operate optimally this would need to be implemented as another graph that could be updated efficiently from the SLAM graph.

There is also some future work that could be applied to the graph integration of loop closure edges.  Instead of simply using an SE(3) edge, omni camera features could be directly added to the graph as shared observations of landmarks, and the loop closure could then be calculated as per a bundle adjustment using reprojection error.  Taking this one step further, if the data association could be maintained across multiple loop closures, then the bundle adjustment would occur across multiple frames, adding even more constraints to existing landmarks.  However at this point it would make more sense to start with a new SLAM system, and add all omni camera and stereo camera observations to the graph all the time, as opposed to running the loop closure pipeline to add omni camera features to the map.

Another option to explore would be how to tightly integrate stereo and omni cameras in the frontend for improved accuracy and robustness of visual odometry.  The results in this work demonstrated quite clearly that the visual odometry approach in ScaViSLAM is not very robust.  ScaViSLAM used a dense stereo tracking approach, which produces accurate results in ideal conditions, however was not robust against moving objects within the scene, or disturbances such as lense flare.  One could combine dense tracking from stereo with visual odometry from the omni camera to handle this particular condition.  Another possibility would be to track points with the stereo camera, and continue to track these points with the omni camera as they go out of frame of the stereo camera, allowing feature tracks to be much longer.  This should provide a more accurate and stable visual odometry.

Finally another more practical line of work would be to pursue sensor fusion with other sensors, such as GPS or inertial sensors.  This would greatly help address such issues as scale estimation or metric consistency of large maps.


