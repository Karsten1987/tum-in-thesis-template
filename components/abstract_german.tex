\selectlanguage{german}

\clearemptydoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Zusammenfassung}

\vspace*{1cm}
\begin{center}
{\Large \bf Zusammenfassung}
\end{center}
\vspace{1cm}

Lokalisierung ist Schluesselkomponente fuer mobileautonome Roboter. Visuelle Lokalisierung hat als kosteneffektive Lösung viel Potenzial, muss jedoch für den kontinuierlichen Einsatz im realen Umfeld effizienter und robuster werden. Diese Arbeit praesentiert einen visuellen Lokalisierungsansatz, bei dem Stereokameradaten und omnidirektionale Kameradaten fusioniert werden. Der Ansatz nutzt komplementäre Eigenschaften der verschiedenen Kameratypen, um die Robustheit und Genauigkeit der Lokalisierung zu erhoehen. Loop Closures, die ueber die omnidirektionale Kamera erkannt werden, werden in den SLAM Graph eines bestehenden Stereo SLAM Systems integriert. Die Verwendung von Monokameradaten zur Berechnung von Transformationsparameter fuer die detektierten Loop Closures fuehrt zu einem Skalierungsproblem. Es werden zwei verschiedene Algorithmen vorgestellt und verglichen, die Stereokameradaten und omnidirektionale Kameradaten kombinieren, um dieses Skalierungsproblem zu lösen. Weiterhin wird eine Methode zur automatischen extrinsischen Kalibrierung der vorgesehenen Kamerakonfiguration vorgestellt. Diese Kalibrierung basiert auf nicht-linearer Graphoptimierung und minimiert den Reprojektionsfehler von automatisch detektierten Landmarken, um automatisch und effizient eine hochgenaue Transformation zwischen den Kameras zu bestimmen. Zusaetzlich wird eine umfassende Evaluierung des gesamten Systems durchgeführt, die das vorliegende System mit einem state-of-the-art Stereo SLAM System vergleicht. Der Vergleich zeigt eine bis zu 82-prozentige Steigerung der Trajektoriengenauigkeit.

\selectlanguage{english}
